\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{multirow}

\title{Assignment 5}
\author{Rok Kos, Ga≈°per Bertoncelj}

\begin{document}
	
	\maketitle
	
	\section*{Local Search}
	
		The local search algorithm accepts three arguments, costs of elements, initial solution and list of subsets respectively. It iterates until local minimum is found. In each iteration it generates neighborhood, which is generated by adding and removing subsets (it also checks if each generated solution in the neighborhood is valid). After that, the cost is calculated for each individual element in the neighborhood. The element with minimal cost is then selected. If that element's cost is smaller than the current smallest cost, the current best element is updated. At the end, best element is returned, along with the indexes of the subsets that were chosen.
		\\ \\
		There is also a function, called \textit{runLocalSearch}. It accepts two arguments, the name of the input file and number of iterations respectively. The purpose of this function is to run the local search algorithm specified number of times (second argument). It collects all the results, and at the end, returns the best one.
		\\ \\	
		In all test cases the Local Search algorithm got the best results. The more random the generation of random solution was, the better solutions it found.
		\\
		
		\begin{center}
			\begin{tabular}{ |l|l|l| }
				\hline
				\multicolumn{3}{ |c| }{\textbf{Local Search Results}} \\
				\hline
				\textbf{Problem} & \textbf{Cost} & \textbf{Chosen Indexes} \\ \hline
				1 & 62.2075 & 1 5 8 10 11 13 14 15 16 19 24 30 31 33 35 \\ \hline
				2 & 1657.733 & 1 2 4 10 14 19 21 28 30 32 33 35 \\ \hline
				3 & 1030.03 & 4 5 8 10 11 12 13 16 19 20 21 24 26 34 36 37 39 41 43 48 \\ \hline
				& & 50 51 54 55 57 58 63 67 70 \\ \hline
				4 & 1423.358 & 15 16 17 29 30 33 38 41 48 50 52 54 55 67 68 72 73 77 79 85 \\ \hline
				& & 96 98 103 104 106 109 113 115 124 125 128 130 132 135 136 137 138 \\ \hline
				5 & 1506.822 & 6 9 16 18 21 24 26 27 30 33 35 37 38 39 41 43 50 \\ \hline
				6 & 73711.88 & 1 4 5 6 8 10 13 14 18 21 24 25 29 35 38 44 47 50 62 66 \\ \hline
				& & 75 80 \\ \hline
				7 & 2724.603 & 3 5 8 10 13 14 18 21 22 26 27 28 30 31 35 37 38 39 41 42 \\ \hline
				& & 45 46 47 50 56 62 65 66 68 70 73 74 78 83 84 86 87 89 93 95 \\ \hline
				& & 97 101 102 103 \\ \hline
				8 & 16237.96 & 4 5 6 9 11 27 32 34 41 42 43 49 53 56 62 68 71 82 84 85 \\ \hline
				& & 105 116 119 130 146 149 151 162 163 164 167 168 192 205 210 215 \\ \hline
				& & 231 234 236 239 254 260 262 265 266 272 273 276 277 278 281 284 \\ \hline
				& & 290 292 301 305 309 315 316 322 325 327 330 333 344 350 360 365 \\ \hline
				& & 381 387 393 398 400 403 407 410 412 415 416 418 420 426 427 438 \\ \hline
				& & 440 441 454 457 459 461 466 477 478 484 487 509 515 521 528 530 \\ \hline
				& & 536 547 549 554 560 568 571 572 574 576 577 585 598 607 618 626 \\ \hline
				& & 637 642 643 648 649 654 657 658 661 662 665 672 677 680 682 694 \\ \hline
				& & 695 696 697 699 700 714 723 742 743 750 756 762 774 776 780 783 \\ \hline
				& & 790 791 803 807 808 812 814 816 819 828 831 838 841 842 847 848 \\ \hline
				& & 852 855 856 867 872 876 877 879 880 882 883 884 891 895 897 900 \\ \hline
				& & 907 908 909 911 920 921 922	924 925 928 930 931 936 942 943 949 \\ \hline
				& & 952 954 963 967 973 977 980 981 985 988 991 992 993 995 1000 \\ \hline
				9 & 22915.93 & 1 3 5 10 13 14 17 18 21 22 23 26 28 30 31 35 41 42 43 44 \\ \hline
				& & 45 46 48 50 51 53 54 55 57 60 63 64 65 66 67 72 73 78 79 80 \\ \hline
				& & 84 85 86 87 88 97 99 101 103 \\ \hline
				10 & 1021.762 & 8 16 22 23 \\ \hline		
			\end{tabular}
		\end{center}
		
	\section*{Tabu Search}
	
		The tabu search algorithm accepts six arguments. They are costs of elements, list of subsets, initial solution, the objective function, maximum number of iterations and size of tabu list respectively. In each iteration, the neighborhood of the current best solution is generated. It then compares each element of the neighborhood with the current best solution, and if the value of objective function of that element is smaller than the value of objective function of the current best solution, that solution is updated. The objective function just calculates the cost of each individual solution
		\\ \\
		In the file \textit{tabuSearchTest.R} we can test the tabu search algorithm. It simply runs multiple iterations and returns the best result.
		\\ \\
		The results from tabu search were mostly not as good as as the ones we got from local search and were often similar to the ones from simulated annealing algorithm. This was probably the case due to quite strict and narrow tabu list.
		\\
	
		\begin{center}
			\begin{tabular}{ |l|l|l| }
				\hline
				\multicolumn{3}{ |c| }{\textbf{Tabu Search Results}} \\
				\hline
				\textbf{Problem} & \textbf{Cost} & \textbf{Chosen Indexes} \\ \hline
				1 & & \\ \hline
				2 & & \\ \hline
				3 & & \\ \hline
				4 & & \\ \hline
				5 & & \\ \hline
				6 & & \\ \hline
				7 & & \\ \hline
				8 & & \\ \hline
				9 & & \\ \hline
				10 & & \\ \hline		
			\end{tabular}
		\end{center}
	
	\section*{Simulated Annealing}
	
		The simulated annealing algorithm also expects six arguments. They are costs of elements, initial solution, list of subsets, initial temperature, lambda factor and maximum number of iterations. In each iteration it firstly generates the neighborhood. Then it randomly selects one solution from that neighborhood. If that random solution's cost is smaller than the current best cost, the current best solution is updated. Then it also compares the random solution's cost with the other solution which is also kept. Again it updates the other solution if the random one is smaller. Otherwise we still make the update, but with some probability. This means, that the other solution is updated quite frequently because the probability is high most of the time. At the end pure local search is also run on the solution that simulated annealing algorithm generated.
		\\ \\
		There is also the \textit{runSimulatedAnnealing} function, which is also used to run the simulated annealing function multiple times, and returns the best result found.
		\\
		
		\begin{center}
			\begin{tabular}{ |l|l|l| }
				\hline
				\multicolumn{3}{ |c| }{\textbf{Simulated Annealing Results}} \\
				\hline
				\textbf{Problem} & \textbf{Cost} & \textbf{Chosen Indexes} \\ \hline
				1 & 62.2075 & 1 5 8 10 11 13 14 15 16 19 24 30 31 33 35 \\ \hline
				2 & 1657.733 & 1  2  4 10 14 19 21 28 30 32 33 35 \\ \hline
				3 & 1030.03 & 4 5 8 10 11 12 13 16 19 20 21 24 26 34 36 37 39 41 43 48 \\ \hline
				& & 50 51 54 55 57 58 63 67 70 \\ \hline
				4 & 1523.658 & 15 16 17 18 19 29 30 38 41 43 48 52 54 60 65 67 68 69 72 73 \\ \hline
				& & 74 77 79 81 85 93 96 100 102 103 109 110 113 115 117 125 134 \\ \hline
				& & 135 137 138 \\ \hline
				5 & 1499.943 & 6 7 9 10 13 18 19 21 25 26 30 31 33 34 37 38 43 44 \\ \hline
				6 & 75897.77 & 1  5  8 13 22 24 25 29 30 31 33 38 39 41 43 44 45 47 58 64 \\ \hline
				& & 68 70 75 80 \\ \hline
				7 & 2786.006 & 1 3 5 10 12 13 15 17 18 19 21 22 26 30 32 35 37 38 39 41 \\ \hline
				& & 42 45 46 47 49 50 53 56 65 66 70 71 73 75 77 82 86 87 89 93 \\ \hline
				& & 94 95 98 100 101 102 103 \\ \hline
				8 & & \\ \hline
				9 & 22921.38 & 1 3 5 10 13 14 17 18 21 22 23 26 28 30 31 35 40 41 42 43 \\ \hline
				& & 44 46 48 50 51 53 54 55 57 60 63 64 65 66 67 72 73 78 79 80 \\ \hline
				& & 84 85 86 87 88 97 99 101 103 \\ \hline
				10 & 1068.837 & 1 8 11 29 \\ \hline		
			\end{tabular}
		\end{center}
	
	\section*{Summary}
	
		Local search algorithm was by far the best algorithm, as it produced pretty good results quite frequently. The first version of local search function that we made only returned the lowest cost it found without the indexes of subsets that were chosen. That is why in some test cases the solutions that we found were better than the ones in the table, we just haven't saved the indexes of the subsets that were chosen.
		
\end{document}